<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Execution Engine &mdash; Intel® Neural Compressor  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Intel® Neural Compressor
          </a>
              <div class="version">
                1.11
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../README.html">Introduction to Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial.html">Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../security_policy.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Execution Engine</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/docs/engine.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="execution-engine">
<h1>Execution Engine<a class="headerlink" href="#execution-engine" title="Permalink to this headline">¶</a></h1>
<p>A reference deep learning execution engine for quantized and sparsified models.</p>
<div class="section" id="deployment-architecture">
<h2>Deployment Architecture<a class="headerlink" href="#deployment-architecture" title="Permalink to this headline">¶</a></h2>
<p>The reference execution engine supports model optimizer, model executor and high performance kernel for CPU.</p>
<a target="_blank" href="docs/imgs/engine_infrastructure.png">
  <img src="imgs/engine_infrastructure.png" alt="Infrastructure" width=762 height=672>
</a></div>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>Just support Linux operating system for now.</p>
<div class="section" id="prepare-environment-and-requirement">
<h3>0. Prepare environment and requirement<a class="headerlink" href="#prepare-environment-and-requirement" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># prepare your env</span>
<span class="n">conda</span> <span class="n">create</span> <span class="o">-</span><span class="n">n</span> <span class="o">&lt;</span><span class="n">env</span> <span class="n">name</span><span class="o">&gt;</span> <span class="n">python</span><span class="o">=</span><span class="mf">3.7</span>
<span class="n">conda</span> <span class="n">install</span> <span class="n">cmake</span> <span class="o">--</span><span class="n">yes</span>
<span class="n">conda</span> <span class="n">install</span> <span class="n">absl</span><span class="o">-</span><span class="n">py</span> <span class="o">--</span><span class="n">yes</span>
</pre></div>
</div>
</div>
<div class="section" id="install-neural-compressor">
<h3>1. install neural-compressor<a class="headerlink" href="#install-neural-compressor" title="Permalink to this headline">¶</a></h3>
<p>The reference engine is our bare-metal deployment example of Intel Neural Compressor, just install neural-compressor and generate the binary.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">neural</span><span class="o">-</span><span class="n">compressor</span>
</pre></div>
</div>
</div>
<div class="section" id="install-c-binary-by-deploy-bare-metal-engine">
<h3>2. install C++ binary by deploy bare metal engine<a class="headerlink" href="#install-c-binary-by-deploy-bare-metal-engine" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">&lt;</span><span class="n">nc_folder</span><span class="o">&gt;</span>
<span class="n">git</span> <span class="n">submodule</span> <span class="n">sync</span>
<span class="n">git</span> <span class="n">submodule</span> <span class="n">update</span> <span class="o">--</span><span class="n">init</span> <span class="o">--</span><span class="n">recursive</span>
<span class="n">cd</span> <span class="n">engine</span><span class="o">/</span><span class="n">executor</span>
<span class="n">mkdir</span> <span class="n">build</span>
<span class="n">cd</span> <span class="n">build</span>
<span class="n">cmake</span> <span class="o">..</span>
<span class="n">make</span> <span class="o">-</span><span class="n">j</span>
</pre></div>
</div>
<p>Then in the build folder, you will get the <code class="docutils literal notranslate"><span class="pre">inferencer</span></code>, <code class="docutils literal notranslate"><span class="pre">engine_py.cpython-37m-x86_64-linux-gnu.so</span></code> and <code class="docutils literal notranslate"><span class="pre">libengine.so</span></code>. The first one is used for pure c++ APIs, and the second is used for Python APIs, they all need the <code class="docutils literal notranslate"><span class="pre">libengine.so</span></code>.</p>
</div>
</div>
<div class="section" id="generate-the-bert-model-intermediate-representations-that-are-yaml-and-bin-files">
<h2>Generate the bert model intermediate representations, that are yaml and bin files<a class="headerlink" href="#generate-the-bert-model-intermediate-representations-that-are-yaml-and-bin-files" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">engine.compile</span> <span class="k">import</span> <span class="nb">compile</span>
<span class="n">model</span> <span class="o">=</span> <span class="nb">compile</span><span class="p">(</span><span class="s1">&#39;/path/to/your/model&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;/ir/path&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Now the engine support tensorflow and onnx model conversion.</p>
</div>
<div class="section" id="use-case">
<h2>Use case<a class="headerlink" href="#use-case" title="Permalink to this headline">¶</a></h2>
<div class="section" id="use-the-inferencer-for-dummy-const-data-performance-test">
<h3>1. Use the <code class="docutils literal notranslate"><span class="pre">inferencer</span></code> for dummy/const data performance test<a class="headerlink" href="#use-the-inferencer-for-dummy-const-data-performance-test" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">./inferencer</span> <span class="pre">--config=&lt;the</span> <span class="pre">generated</span> <span class="pre">yaml</span> <span class="pre">file</span> <span class="pre">path&gt;</span> <span class="pre">--weight=&lt;the</span> <span class="pre">generated</span> <span class="pre">bin</span> <span class="pre">file</span> <span class="pre">path&gt;</span> <span class="pre">--batch_size=32</span> <span class="pre">--iterations=20</span></code></p>
<p>You can set the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> and <code class="docutils literal notranslate"><span class="pre">iterations</span></code> number. Besides you can use the <code class="docutils literal notranslate"><span class="pre">numactl</span></code> command to bind cpu cores and open multi-instances. For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS=4</span> <span class="pre">numactl</span> <span class="pre">-C</span> <span class="pre">'0-3'</span> <span class="pre">./inferencer</span> <span class="pre">--config=&lt;the</span> <span class="pre">generated</span> <span class="pre">yaml</span> <span class="pre">file</span> <span class="pre">path&gt;</span> <span class="pre">--weight=&lt;the</span> <span class="pre">generated</span> <span class="pre">bin</span> <span class="pre">file</span> <span class="pre">path&gt;</span> <span class="pre">--batch_size=32</span> <span class="pre">--iterations=20</span></code></p>
<p>Then, you can see  throughput result of the <code class="docutils literal notranslate"><span class="pre">inferencer</span></code>  on the terminal.</p>
<p>Please remember to change the input data type, shape and range for your input in <code class="docutils literal notranslate"><span class="pre">inferencer.cpp</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="n">vector</span><span class="o">&lt;</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">input_dtype</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;int32&quot;</span><span class="p">);</span>
  <span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="nb">float</span><span class="o">&gt;&gt;</span> <span class="n">input_range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">vector</span><span class="o">&lt;</span><span class="nb">float</span><span class="o">&gt;</span><span class="p">({</span><span class="mi">1</span><span class="p">,</span> <span class="mi">300</span><span class="p">}));</span>
  <span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">int64_t</span><span class="o">&gt;&gt;</span> <span class="n">input_shape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">{</span><span class="n">FLAGS_batch_size</span><span class="p">,</span> <span class="n">FLAGS_seq_len</span><span class="p">});</span>
  <span class="n">executor</span><span class="p">::</span><span class="n">DataLoader</span><span class="o">*</span> <span class="n">dataloader</span><span class="p">;</span>
  <span class="o">//</span> <span class="n">dataloader</span> <span class="o">=</span> <span class="n">new</span> <span class="n">executor</span><span class="p">::</span><span class="n">ConstDataLoader</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">input_range</span><span class="p">);</span>
  <span class="n">dataloader</span> <span class="o">=</span> <span class="n">new</span> <span class="n">executor</span><span class="p">::</span><span class="n">DummyDataLoader</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">input_range</span><span class="p">);</span>
</pre></div>
</div>
<p>The dataloader generate data using prepare_batch() and make sure the generate data is the yaml need:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">:</span> 
  <span class="n">name</span><span class="p">:</span> <span class="n">bert_mlperf_int8</span>
  <span class="n">operator</span><span class="p">:</span>
    <span class="n">input_data</span><span class="p">:</span>
      <span class="nb">type</span><span class="p">:</span> <span class="n">Input</span>
      <span class="n">output</span><span class="p">:</span>
        <span class="c1"># -1 means it&#39;s dynamic shape</span>

        <span class="n">input_ids</span><span class="p">:</span>
          <span class="n">dtype</span><span class="p">:</span> <span class="n">int32</span>
          <span class="n">shape</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">segment_ids</span><span class="p">:</span> 
          <span class="n">dtype</span><span class="p">:</span> <span class="n">int32</span>
          <span class="n">shape</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">input_mask</span><span class="p">:</span>
          <span class="n">dtype</span><span class="p">:</span> <span class="n">int32</span>
          <span class="n">shape</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">softmax1_min</span><span class="p">:</span>
          <span class="n">dtype</span><span class="p">:</span> <span class="n">fp32</span>
          <span class="n">shape</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
          <span class="n">location</span><span class="p">:</span> <span class="p">[</span><span class="mi">430411380</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
</pre></div>
</div>
<p>All input tensors are in an operator typed Input. But slightly the difference is a weight or frozen (constant) tensor has location defined in bin file, while an activation or input doesn’t have location. When you use C++ interface, initialize the tensor config and feed data/shape from dataloader:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="o">//</span> <span class="n">initialize</span> <span class="n">the</span> <span class="nb">input</span> <span class="n">tensor</span> <span class="n">config</span><span class="p">(</span><span class="n">which</span> <span class="n">correspond</span> <span class="n">to</span> <span class="n">the</span> <span class="n">yaml</span> <span class="n">tensor</span> <span class="n">without</span> <span class="n">location</span><span class="p">)</span>
  <span class="n">vector</span><span class="o">&lt;</span><span class="n">executor</span><span class="p">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">input_tensors</span><span class="p">;</span>
  <span class="n">auto</span> <span class="n">input_configs</span> <span class="o">=</span> <span class="n">bert_model</span><span class="o">.</span><span class="n">input_configs</span><span class="p">();</span>
  <span class="k">for</span> <span class="p">(</span><span class="nb">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">bert_model</span><span class="o">.</span><span class="n">num_inputs</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">input_tensors</span><span class="o">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">executor</span><span class="p">::</span><span class="n">Tensor</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">input_configs</span><span class="p">[</span><span class="n">i</span><span class="p">])));</span>
  <span class="o">//</span> <span class="n">feed</span> <span class="n">the</span> <span class="n">data</span> <span class="ow">and</span> <span class="n">shape</span>
  <span class="n">auto</span> <span class="n">raw_data</span> <span class="o">=</span> <span class="n">dataloader</span><span class="o">-&gt;</span><span class="n">prepare_batch</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
  <span class="k">for</span> <span class="p">(</span><span class="nb">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">input_tensors</span><span class="o">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">input_tensors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">raw_data</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="n">input_tensors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="p">}</span>
</pre></div>
</div>
<p>The output tensor is defined in an operator named Output, which only have inputs. See an example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="n">output_data</span><span class="p">:</span>
    <span class="nb">type</span><span class="p">:</span> <span class="n">Output</span>
    <span class="nb">input</span><span class="p">:</span>
      <span class="n">matmul_post_output_reshape</span><span class="p">:</span> <span class="p">{}</span>
</pre></div>
</div>
<p>You can add the tensor you want to the Output. Remember, all output tensors from operators should have operators take as input, that means output edges should have an end node. In this case, each operator’s output has one or several other operators take as input.</p>
<p>If you want to close log information of the <code class="docutils literal notranslate"><span class="pre">inferencer</span></code>, use the command <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">GLOG_minloglevel=2</span></code> before executing the <code class="docutils literal notranslate"><span class="pre">inferencer</span></code>.  <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">GLOG_minloglevel=1</span></code> will open the log information again. This command can also be used to run the model with Python APIs.</p>
</div>
<div class="section" id="use-the-python-apis">
<h3>2. Use the python APIs<a class="headerlink" href="#use-the-python-apis" title="Permalink to this headline">¶</a></h3>
<p>If you use pip install -e . to install the execution engine in your current folder, please make sure to export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/your/libengine.so</p>
<p>Other installation method you can skip the step.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># import the model</span>
<span class="kn">from</span> <span class="nn">engine_py</span> <span class="k">import</span> <span class="n">Model</span>
<span class="c1"># load the model</span>
<span class="c1"># config_path is the generated yaml file path</span>
<span class="c1"># weight_path is the generated bin file path</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="n">weight_path</span><span class="p">)</span>
<span class="c1"># use model to do inference</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">])</span>
</pre></div>
</div>
<p>Python API support input numpy array and output numpy array. if you have several inputs, you can put them in to a list and feed to the model forward interface.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">input_ids</span></code>, <code class="docutils literal notranslate"><span class="pre">segment_ids</span></code> and <code class="docutils literal notranslate"><span class="pre">input_mask</span></code> are the input numpy array data of a bert model, which have size (batch_size, seq_len). Note that the <code class="docutils literal notranslate"><span class="pre">out</span></code> is a list contains the bert model output numpy data (<code class="docutils literal notranslate"><span class="pre">out=[output</span> <span class="pre">numpy</span> <span class="pre">data]</span></code>).</p>
</div>
</div>
<div class="section" id="get-a-low-precision-model-using-neural-compressor-tool">
<h2>Get a low precision model using neural_compressor tool<a class="headerlink" href="#get-a-low-precision-model-using-neural-compressor-tool" title="Permalink to this headline">¶</a></h2>
<p>It’s easy to convert a tensorflow or onnx model to the int8 ir with high performance.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neural_compressor.experimental</span> <span class="k">import</span> <span class="n">Quantization</span><span class="p">,</span> <span class="n">common</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">TF_BERTDataSet</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">do_lower_case</span><span class="p">)</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantization</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">input_model</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">eval_dataloader</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">calib_dataloader</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">q_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">q_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_model</span><span class="p">)</span>
</pre></div>
</div>
<p>The output_model is the generated int8 ir of the execution engine. You are encouraged to test the benchmark in a bare-metal way.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neural_compressor.experimental</span> <span class="k">import</span> <span class="n">Benchmark</span><span class="p">,</span> <span class="n">common</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">TF_BERTDataSet</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">do_lower_case</span><span class="p">)</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">Benchmark</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
<span class="n">evaluator</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">input_model</span>
<span class="n">evaluator</span><span class="o">.</span><span class="n">b_dataloader</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">evaluator</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>
</pre></div>
</div>
<p>Reference examples can be found at &lt;nc_folder&gt;/examples/engine/nlp</p>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>