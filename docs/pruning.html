<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Pruning &mdash; Intel® Neural Compressor  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Benchmarking" href="benchmark.html" />
    <link rel="prev" title="Dynamic Quantization" href="dynamic_quantization.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Intel® Neural Compressor
          </a>
              <div class="version">
                1.11
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../README.html">Introduction to Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial.html">Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="doclist.html">Developer Documentation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="doclist.html#get-started">Get Started</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="doclist.html#deep-dive">Deep Dive</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="Quantization.html">Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="PTQ.html">PTQ</a></li>
<li class="toctree-l3"><a class="reference internal" href="QAT.html">QAT</a></li>
<li class="toctree-l3"><a class="reference internal" href="dynamic_quantization.html">Dynamic Quantization</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Pruning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pruning-algorithms-supported-by-neural-compressor">Pruning Algorithms supported by Neural Compressor</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pruning-api">Pruning API</a></li>
<li class="toctree-l4"><a class="reference internal" href="#examples">Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="benchmark.html">Benchmarking</a></li>
<li class="toctree-l3"><a class="reference internal" href="mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="graph_optimization.html">Graph Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_conversion.html">Model Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensorboard.html">TensorBoard</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="doclist.html#advanced-topics">Advanced Topics</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../security_policy.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="doclist.html">Developer Documentation</a> &raquo;</li>
      <li>Pruning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/docs/pruning.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="pruning">
<h1>Pruning<a class="headerlink" href="#pruning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Network pruning is one of popular approaches of network compression, which reduces the size of a network by removing parameters with minimal drop in accuracy.</p>
<ul class="simple">
<li><p>Structured Pruning</p></li>
</ul>
<p>Structured pruning means pruning sparsity patterns, in which there is some structure, most often in the form of blocks.
Neural Compressor provided a NLP Structured pruning example:
<a class="reference external" href="https://github.com/intel/neural-compressor/tree/0e4296149dc73518156fe21593d691990a1ce9c0/docs/../examples/pytorch/nlp/huggingface_models/question-answering/pruning/group_lasso/eager">Bert example</a>.
<a class="reference external" href="../examples/pytorch/nlp/huggingface_models/question-answering/pruning/group_lasso/eager/README">README of Structured pruning example</a>.</p>
<ul class="simple">
<li><p>Unstructured Pruning</p></li>
</ul>
<p>Unstructured pruning means pruning unstructured sparsity (aka random sparsity) patterns, where the nonzero patterns are irregular and could be anywhere in the matrix.</p>
<ul class="simple">
<li><p>Filter/Channel Pruning</p></li>
</ul>
<p>Filter/Channel pruning means pruning a larger part of the network, such as filters or layers, according to some rules.</p>
</div>
<div class="section" id="pruning-algorithms-supported-by-neural-compressor">
<h2>Pruning Algorithms supported by Neural Compressor<a class="headerlink" href="#pruning-algorithms-supported-by-neural-compressor" title="Permalink to this headline">¶</a></h2>
<table border="1" class="docutils">
<thead>
<tr>
<th>Pruning Type</th>
<th>Algorithm</th>
<th>PyTorch</th>
<th>Tensorflow</th>
</tr>
</thead>
<tbody>
<tr>
<td>unstructured pruning</td>
<td>basic_magnitude</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td></td>
<td>pattern_lock</td>
<td>Yes</td>
<td>N/A</td>
</tr>
<tr>
<td>structured pruning</td>
<td>pattern_lock</td>
<td>Yes</td>
<td>N/A</td>
</tr>
<tr>
<td>filter/channel pruning</td>
<td>gradient_sensitivity</td>
<td>Yes</td>
<td>N/A</td>
</tr>
</tbody>
</table><p>Neural Compressor also supports the two-shot execution of unstructured pruning and post-training quantization.</p>
<ul class="simple">
<li><p>basic_magnitude:</p>
<ul>
<li><p>The algorithm prunes the weight by the lowest absolute value at each layer with given sparsity target.</p></li>
</ul>
</li>
<li><p>gradient_sensitivity:</p>
<ul>
<li><p>The algorithm prunes the head, intermediate layers, and hidden states in NLP model according to importance score calculated by following the paper <a class="reference external" href="https://arxiv.org/abs/2010.13382">FastFormers</a>.</p></li>
</ul>
</li>
<li><p>pattern_lock</p>
<ul>
<li><p>The algorithm takes a sparsity model as input and starts to fine tune this sparsity model and locks the sparsity pattern by freezing those zero values in weight tensor after weight update during training.</p></li>
</ul>
</li>
<li><p>pruning and then post-training quantization</p>
<ul>
<li><p>The algorithm executes unstructured pruning and then executes post-training quantization.</p></li>
</ul>
</li>
<li><p>pruning during quantization-aware training</p>
<ul>
<li><p>The algorithm executes unstructured pruning during quantization-aware training.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="pruning-api">
<h2>Pruning API<a class="headerlink" href="#pruning-api" title="Permalink to this headline">¶</a></h2>
<div class="section" id="user-facing-api">
<h3>User facing API<a class="headerlink" href="#user-facing-api" title="Permalink to this headline">¶</a></h3>
<p>Neural Compressor pruning API is defined under <code class="docutils literal notranslate"><span class="pre">neural_compressor.experimental.Pruning</span></code>, which takes a user defined yaml file as input. The user defined yaml defines training, pruning and evaluation behaviors.
<a class="reference internal" href="pruning_api.html"><span class="doc">API Readme</span></a>.</p>
</div>
<div class="section" id="launcher-code">
<h3>Launcher code<a class="headerlink" href="#launcher-code" title="Permalink to this headline">¶</a></h3>
<p>Simplest launcher code if training behavior is defined in user-defined yaml.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neural_compressor.experimental</span> <span class="k">import</span> <span class="n">Pruning</span><span class="p">,</span> <span class="n">common</span>
<span class="n">prune</span> <span class="o">=</span> <span class="n">Pruning</span><span class="p">(</span><span class="s1">&#39;/path/to/user/pruning/yaml&#39;</span><span class="p">)</span>
<span class="n">prune</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prune</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>Pruning class also support Pruning_Conf class as it’s argument.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lpot.experimental</span> <span class="k">import</span> <span class="n">Pruning</span><span class="p">,</span> <span class="n">common</span>
<span class="kn">from</span> <span class="nn">lpot.conf.config</span> <span class="k">import</span> <span class="n">Pruning_Conf</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">Pruning_Conf</span><span class="p">(</span><span class="s1">&#39;/path/to/user/pruning/yaml&#39;</span><span class="p">)</span>
<span class="n">prune</span> <span class="o">=</span> <span class="n">Pruning</span><span class="p">(</span><span class="n">conf</span><span class="p">)</span>
<span class="n">prune</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prune</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="user-defined-yaml">
<h3>User-defined yaml<a class="headerlink" href="#user-defined-yaml" title="Permalink to this headline">¶</a></h3>
<p>The user-defined yaml follows below syntax, note <code class="docutils literal notranslate"><span class="pre">train</span></code> section is optional if user implements <code class="docutils literal notranslate"><span class="pre">pruning_func</span></code> and sets to <code class="docutils literal notranslate"><span class="pre">pruning_func</span></code> attribute of pruning instance.
<a class="reference external" href="https://github.com/intel/neural-compressor/blob/0e4296149dc73518156fe21593d691990a1ce9c0/docs/../docs/pruning.yaml">user-defined yaml</a>.</p>
<div class="section" id="train">
<h4><code class="docutils literal notranslate"><span class="pre">train</span></code><a class="headerlink" href="#train" title="Permalink to this headline">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">train</span></code> section defines the training behavior, including what training hyper-parameter would be used and which dataloader is used during training.</p>
</div>
<div class="section" id="approach">
<h4><code class="docutils literal notranslate"><span class="pre">approach</span></code><a class="headerlink" href="#approach" title="Permalink to this headline">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">approach</span></code> section defines which pruning algorithm is used and how to apply it during training process.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">weight</span> <span class="pre">compression</span></code>: pruning target, currently only <code class="docutils literal notranslate"><span class="pre">weight</span> <span class="pre">compression</span></code> is supported. <code class="docutils literal notranslate"><span class="pre">weight</span> <span class="pre">compression</span></code> means zeroing the weight matrix. The parameters for <code class="docutils literal notranslate"><span class="pre">weight</span> <span class="pre">compression</span></code> is divided into global parameters and local parameters in different <code class="docutils literal notranslate"><span class="pre">pruners</span></code>. Global parameters may contain <code class="docutils literal notranslate"><span class="pre">start_epoch</span></code>, <code class="docutils literal notranslate"><span class="pre">end_epoch</span></code>, <code class="docutils literal notranslate"><span class="pre">initial_sparsity</span></code>, <code class="docutils literal notranslate"><span class="pre">target_sparsity</span></code> and <code class="docutils literal notranslate"><span class="pre">frequency</span></code>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">start_epoch</span></code>:  on which epoch pruning begins</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">end_epoch</span></code>: on which epoch pruning ends</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">initial_sparsity</span></code>: initial sparsity goal, default 0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">target_sparsity</span></code>: target sparsity goal</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">frequency</span></code>: frequency to updating sparsity</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Pruner</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">prune_type</span></code>: pruning algorithm, currently <code class="docutils literal notranslate"><span class="pre">basic_magnitude</span></code> and <code class="docutils literal notranslate"><span class="pre">gradient_sensitivity</span></code> are supported.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">names</span></code>: weight name to be pruned. If no weight is specified, all weights of the model will be pruned.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">parameters</span></code>: Additional parameters is required <code class="docutils literal notranslate"><span class="pre">gradient_sensitivity</span></code> prune_type, which is defined in <code class="docutils literal notranslate"><span class="pre">parameters</span></code> field. Those parameters determined how a weight is pruned, including the pruning target and the calculation of weight’s importance. it contains:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">target</span></code>: the pruning target for weight.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stride</span></code>: each stride of the pruned weight.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transpose</span></code>: whether to transpose weight before prune.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">normalize</span></code>: whether to normalize the calculated importance.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">index</span></code>: the index of calculated importance.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">importance_inputs</span></code>: inputs of the importance calculation for weight.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">importance_metric</span></code>: the metric used in importance calculation, currently <code class="docutils literal notranslate"><span class="pre">abs_gradient</span></code> and <code class="docutils literal notranslate"><span class="pre">weighted_gradient</span></code> are supported.</p></li>
</ul>
<p>Take above as an example, if we assume the ‘bert.encoder.layer.0.attention.output.dense.weight’ is the shape of [N, 12*64]. The target 8 and stride 64 is used to control the pruned weight shape to be [N, 8*64]. <code class="docutils literal notranslate"><span class="pre">Transpose</span></code> set to True indicates the weight is pruned at dim 1 and should be transposed to [12*64, N] before pruning. <code class="docutils literal notranslate"><span class="pre">importance_input</span></code> and <code class="docutils literal notranslate"><span class="pre">importance_metric</span></code> specify the actual input and metric to calculate importance matrix.</p>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="pruning-with-user-defined-pruning-func">
<h3>Pruning with user-defined pruning_func()<a class="headerlink" href="#pruning-with-user-defined-pruning-func" title="Permalink to this headline">¶</a></h3>
<p>User can pass the customized training/evaluation functions to <code class="docutils literal notranslate"><span class="pre">Pruning</span></code> for flexible scenarios. <code class="docutils literal notranslate"><span class="pre">Pruning</span></code>  In this case, pruning process can be done by pre-defined hooks in Neural Compressor. User needs to put those hooks inside the training function.</p>
<p>Neural Compressor defines several hooks for user pass</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">on_epoch_begin</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span> <span class="p">:</span> <span class="n">Hook</span> <span class="n">executed</span> <span class="n">at</span> <span class="n">each</span> <span class="n">epoch</span> <span class="n">beginning</span>
<span class="n">on_batch_begin</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="p">:</span> <span class="n">Hook</span> <span class="n">executed</span> <span class="n">at</span> <span class="n">each</span> <span class="n">batch</span> <span class="n">beginning</span>
<span class="n">on_batch_end</span><span class="p">()</span> <span class="p">:</span> <span class="n">Hook</span> <span class="n">executed</span> <span class="n">at</span> <span class="n">each</span> <span class="n">batch</span> <span class="n">end</span>
<span class="n">on_epoch_end</span><span class="p">()</span> <span class="p">:</span> <span class="n">Hook</span> <span class="n">executed</span> <span class="n">at</span> <span class="n">each</span> <span class="n">epoch</span> <span class="n">end</span>
</pre></div>
</div>
<p>Following section shows how to use hooks in user pass-in training function which is part of example from BERT training:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pruning_func</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_train_epochs</span><span class="p">)):</span>
        <span class="n">pbar</span> <span class="o">=</span> <span class="n">ProgressBar</span><span class="p">(</span><span class="n">n_total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s1">&#39;Training&#39;</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">prune</span><span class="o">.</span><span class="n">on_epoch_begin</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
            <span class="n">prune</span><span class="o">.</span><span class="n">on_batch_begin</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;input_ids&#39;</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                      <span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                      <span class="s1">&#39;labels&#39;</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">3</span><span class="p">]}</span>
            <span class="c1">#inputs[&#39;token_type_ids&#39;] = batch[2]</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># model outputs are always tuple in transformers (see doc)</span>

            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">n_gpu</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># mean() to average on multi-gpu parallel training</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span>

            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">args</span><span class="o">.</span><span class="n">max_grad_norm</span><span class="p">)</span>

            
            <span class="k">if</span> <span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Update learning rate schedule</span>
                <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    
            <span class="n">prune</span><span class="o">.</span><span class="n">on_batch_end</span><span class="p">()</span>
<span class="o">...</span>
</pre></div>
</div>
<p>In this case, the launcher code is like the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neural_compressor.experimental</span> <span class="kn">import</span> <span class="n">Pruning</span><span class="p">,</span> <span class="n">common</span>
<span class="n">prune</span> <span class="o">=</span> <span class="n">Pruning</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
<span class="n">prune</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
<span class="n">prune</span><span class="o">.</span><span class="n">pruning_func</span> <span class="o">=</span> <span class="n">pruning_func</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prune</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="scheduler-for-pruning-and-quantization">
<h3>Scheduler for Pruning and Quantization<a class="headerlink" href="#scheduler-for-pruning-and-quantization" title="Permalink to this headline">¶</a></h3>
<p>Neural Compressor defined Scheduler to automatically pipeline execute prune and post-training quantization. After appending separate component into scheduler pipeline, scheduler executes them one by one. In following example it executes the pruning and then post-training quantization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neural_compressor.experimental</span> <span class="kn">import</span> <span class="n">Quantization</span><span class="p">,</span> <span class="n">common</span><span class="p">,</span> <span class="n">Pruning</span><span class="p">,</span> <span class="n">Scheduler</span>
<span class="n">prune</span> <span class="o">=</span> <span class="n">Pruning</span><span class="p">(</span><span class="n">prune_conf</span><span class="p">)</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantization</span><span class="p">(</span><span class="n">post_training_quantization_conf</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">Scheduler</span><span class="p">()</span>
<span class="n">scheduler</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
<span class="n">scheduler</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prune</span><span class="p">)</span>
<span class="n">scheduler</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">quantizer</span><span class="p">)</span>
<span class="n">opt_model</span> <span class="o">=</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h2>
<div class="section" id="examples-in-neural-compressor">
<h3>Examples in Neural Compressor<a class="headerlink" href="#examples-in-neural-compressor" title="Permalink to this headline">¶</a></h3>
<p>Following examples are supported in Neural Compressor:</p>
<ul class="simple">
<li><p>CNN Examples:</p>
<ul>
<li><p><a class="reference external" href="../examples/pytorch/image_recognition/torchvision_models/pruning/magnitude/eager/README">resnet example</a>: magnitude pruning on resnet.</p></li>
<li><p><a class="reference external" href="../examples/pytorch/image_recognition/torchvision_models/optimization_pipeline/prune_and_ptq/eager/README">pruning and post-training quantization</a>: magnitude pruning and then post-training quantization on resnet.</p></li>
<li><p><a class="reference external" href="../examples/tensorflow/image_recognition/resnet_v2/pruning/magnitude/README">resnet_v2 example</a>: magnitude pruning on resnet_v2 for tensorflow.</p></li>
</ul>
</li>
<li><p>NLP Examples:</p>
<ul>
<li><p><a class="reference external" href="../examples/pytorch/nlp/huggingface_models/text-classification/pruning/magnitude/eager/README">BERT example</a>: magnitude pruning on DistilBERT.</p></li>
<li><p><a class="reference external" href="../examples/pytorch/nlp/huggingface_models/text-classification/pruning/pattern_lock/eager/README">BERT example</a>: Pattern-lock and head-pruning on BERT-base.</p></li>
</ul>
</li>
</ul>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="dynamic_quantization.html" class="btn btn-neutral float-left" title="Dynamic Quantization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="benchmark.html" class="btn btn-neutral float-right" title="Benchmarking" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>