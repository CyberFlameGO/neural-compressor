:orphan:

:py:mod:`neural_compressor`
===========================

.. py:module:: neural_compressor


Subpackages
-----------
.. toctree::
   :titlesonly:
   :maxdepth: 3

   data/index.rst
   experimental/index.rst
   metric/index.rst
   pruner/index.rst
   ux/index.rst


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   pruning/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   neural_compressor.Pruning
   neural_compressor.Benchmark
   neural_compressor.DistillationConfig




.. py:class:: Pruning(config)

   Pruning.

   The main class that users will used in codes to do pruning.
   Contain at least one Pruner object.

   :param config: a string. The path to a config file. For config file template, please refer to
                  https://github.com/intel/neural-compressor/tree/master/examples/pytorch/nlp/huggingface_models/text-classification/pruning/pytorch_pruner/eager/

   .. attribute:: model

      The model object to prune.

   .. attribute:: config_file_path

      A string. The path to a config file.

   .. attribute:: pruners

      A list. A list of Pruner objects.

   .. attribute:: pruner_info

      A config dict object. Contains pruners' information.

   .. py:method:: update_config(*args, **kwargs)

      Add user-defined arguments to the original configurations.

      The original config of pruning is read from a file.
      However, users can still modify configurations by passing key-value arguments in this function.
      Please note that the key-value arguments' keys are analysable in current configuration.


   .. py:method:: get_sparsity_ratio()

      Calculate sparsity ratio of a module/layer.

      :returns: Three floats.
                elementwise_over_matmul_gemm_conv refers to zero elements' ratio in pruning layers.
                elementwise_over_all refers to zero elements' ratio in all layers in the model.
                blockwise_over_matmul_gemm_conv refers to all-zero blocks' ratio in pruning layers.


   .. py:method:: on_train_begin()

      Implement at the beginning of training process.

      Before training, ensure that pruners are generated.


   .. py:method:: on_epoch_begin(epoch)

      Implement at the beginning of every epoch.


   .. py:method:: on_step_begin(local_step)

      Implement at the beginning of every step.


   .. py:method:: on_before_optimizer_step()

      Implement before optimizer.step().


   .. py:method:: on_step_end()

      Implement at the end of every step.


   .. py:method:: on_epoch_end()

      Implement the end of every epoch.


   .. py:method:: on_train_end()

      Implement the end of training phase.


   .. py:method:: on_before_eval()

      Implement at the beginning of evaluation phase.


   .. py:method:: on_after_eval()

      Implement at the end of evaluation phase.


   .. py:method:: on_after_optimizer_step()

      Implement after optimizer.step().



.. py:class:: Benchmark(conf_fname_or_obj)

   Bases: :py:obj:`object`

   Benchmark class can be used to evaluate the model performance, with the objective
      setting, user can get the data of what they configured in yaml

   :param conf_fname_or_obj: The path to the YAML configuration file or
                             Benchmark_Conf class containing accuracy goal, tuning objective and preferred
                             calibration & quantization tuning space etc.
   :type conf_fname_or_obj: string or obj


.. py:class:: DistillationConfig(teacher_model=None, criterion=criterion, optimizer={'SGD': {'learning_rate': 0.0001}})

   Config of distillation.

   :param teacher_model: Teacher model for distillation. Defaults to None.
   :type teacher_model: Callable
   :param features: Teacher features for distillation, features and teacher_model are alternative.
                    Defaults to None.
   :type features: optional
   :param criterion: Distillation loss configure.
   :type criterion: Callable, optional
   :param optimizer: Optimizer configure.
   :type optimizer: dictionary, optional


