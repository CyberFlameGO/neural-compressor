<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Benchmark an engine IR using C++ API &mdash; Intel® Neural Compressor  documentation</title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Intel® Neural Compressor
          </a>
              <div class="version">
                1.8.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../README.html">Introduction to Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docs/tutorial.html">Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docs/api-introduction.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docs/doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../security_policy.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Benchmark an engine IR using C++ API</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/engine/docs/engine_inferencer.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="benchmark-an-engine-ir-using-c-api">
<h1>Benchmark an engine IR using C++ API<a class="headerlink" href="#benchmark-an-engine-ir-using-c-api" title="Permalink to this headline">¶</a></h1>
<p>A deep learning inference engine for quantized and sparsified models.</p>
<div class="section" id="architecture">
<h2>Architecture<a class="headerlink" href="#architecture" title="Permalink to this headline">¶</a></h2>
<p>Engine support model optimizer, model executor and high performance kernel for multi device.</p>
<a target="_blank" href="docs/imgs/infrastructure.png">
  <img src="docs/imgs/infrastructure.png" alt="Infrastructure" width=800 height=500>
</a></div>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>Just support Linux operating system for now.</p>
<div class="section" id="prepare-environment-and-requirement">
<h3>0.Prepare environment and requirement<a class="headerlink" href="#prepare-environment-and-requirement" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># prepare your env</span>
<span class="n">conda</span> <span class="n">create</span> <span class="o">-</span><span class="n">n</span> <span class="o">&lt;</span><span class="n">env</span> <span class="n">name</span><span class="o">&gt;</span> <span class="n">python</span><span class="o">=</span><span class="mf">3.7</span>
<span class="n">conda</span> <span class="n">install</span> <span class="n">cmake</span> <span class="o">--</span><span class="n">yes</span>
<span class="n">conda</span> <span class="n">install</span> <span class="n">absl</span><span class="o">-</span><span class="n">py</span> <span class="o">--</span><span class="n">yes</span>

<span class="c1"># install tensorflow </span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">storage</span><span class="o">.</span><span class="n">googleapis</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">intel</span><span class="o">-</span><span class="n">optimized</span><span class="o">-</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">intel_tensorflow</span><span class="o">-</span><span class="mf">1.15</span><span class="o">.</span><span class="mi">0</span><span class="n">up2</span><span class="o">-</span><span class="n">cp37</span><span class="o">-</span><span class="n">cp37m</span><span class="o">-</span><span class="n">manylinux2010_x86_64</span><span class="o">.</span><span class="n">whl</span> 

<span class="c1"># install transformers</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span>

<span class="c1"># install INC</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">neural</span><span class="o">-</span><span class="n">compressor</span>
</pre></div>
</div>
</div>
<div class="section" id="clone-the-engine-repo">
<h3>1.Clone the engine repo<a class="headerlink" href="#clone-the-engine-repo" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">clone</span> <span class="pre">https://github.com/intel-innersource/frameworks.ai.deep-engine.intel-deep-engine.git</span> <span class="pre">&lt;work_folder&gt;</span></code></p>
</div>
<div class="section" id="generate-the-bert-model-intermediate-representations-that-are-yaml-and-bin-flies">
<h3>2.Generate the bert model intermediate representations, that are yaml and bin flies<a class="headerlink" href="#generate-the-bert-model-intermediate-representations-that-are-yaml-and-bin-flies" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># activate the env created at the Installation step</span>
<span class="n">conda</span> <span class="n">activate</span> <span class="o">&lt;</span><span class="n">your_env_name</span><span class="o">&gt;</span>
<span class="n">cd</span> <span class="o">&lt;</span><span class="n">work_folder</span><span class="o">&gt;/</span><span class="n">engine</span>
<span class="n">python</span> <span class="n">examples</span><span class="o">/</span><span class="n">convert_bert</span><span class="o">/</span><span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">model</span><span class="o">=&lt;</span><span class="n">your_bert_model_path</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Then in <code class="docutils literal notranslate"><span class="pre">&lt;work_folder&gt;/engine/ir/</span></code>, you will see the corresponding yaml and bin files</p>
</div>
<div class="section" id="build-the-engine-make-sure-your-gcc-version-7-3">
<h3>3.Build the engine, <strong>make sure your gcc version &gt;= 7.3</strong><a class="headerlink" href="#build-the-engine-make-sure-your-gcc-version-7-3" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">&lt;</span><span class="n">work_folder</span><span class="o">&gt;/</span><span class="n">engine</span><span class="o">/</span><span class="n">executor</span>
<span class="n">mkdir</span> <span class="n">build</span>
<span class="n">cd</span> <span class="n">build</span>
<span class="n">cmake</span> <span class="o">..</span>
<span class="n">make</span> <span class="o">-</span><span class="n">j</span>
</pre></div>
</div>
<p>Then in the build folder, you will get the <code class="docutils literal notranslate"><span class="pre">inferencer</span></code>, <code class="docutils literal notranslate"><span class="pre">engine_py.cpython-37m-x86_64-linux-gnu.so</span></code> and <code class="docutils literal notranslate"><span class="pre">libengine.so</span></code>. The first one is used for pure c++ model inference, and the second is used for python inference, they all need the <code class="docutils literal notranslate"><span class="pre">libengine.so</span></code>.</p>
</div>
<div class="section" id="use-the-inferencer-for-dummy-const-data-performance-test">
<h3>4.Use the <code class="docutils literal notranslate"><span class="pre">inferencer</span></code> for dummy/const data performance test<a class="headerlink" href="#use-the-inferencer-for-dummy-const-data-performance-test" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">./inferencer</span> <span class="pre">--config=&lt;the</span> <span class="pre">generated</span> <span class="pre">yaml</span> <span class="pre">file</span> <span class="pre">path&gt;</span> <span class="pre">--weight=&lt;the</span> <span class="pre">generated</span> <span class="pre">bin</span> <span class="pre">file</span> <span class="pre">path&gt;</span> <span class="pre">--batch_size=32</span> <span class="pre">--iterations=20</span></code></p>
<p>You can set the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> and <code class="docutils literal notranslate"><span class="pre">iterations</span></code> number. Besides you can use the <code class="docutils literal notranslate"><span class="pre">numactl</span></code> command to bind cpu cores and open multi-instances. For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS=4</span> <span class="pre">numactl</span> <span class="pre">-C</span> <span class="pre">'0-3'</span> <span class="pre">./inferencer</span> <span class="pre">--config=&lt;the</span> <span class="pre">generated</span> <span class="pre">yaml</span> <span class="pre">file</span> <span class="pre">path&gt;</span> <span class="pre">--weight=&lt;the</span> <span class="pre">generated</span> <span class="pre">bin</span> <span class="pre">file</span> <span class="pre">path&gt;</span> <span class="pre">--batch_size=32</span> <span class="pre">--iterations=20</span></code></p>
<p>Then, you can see throughput result of the <code class="docutils literal notranslate"><span class="pre">inferencer</span></code>  on the terminal.</p>
<p>You can also use our prepared yamls for performance test on <code class="docutils literal notranslate"><span class="pre">&lt;work_folder&gt;/engine/examples/nlp</span></code>, now the supported example yaml table is like below:
| Model Name | FP32 | INT8 | Latency |
| :—— | :—— | :—— | :—— |
| Bert Base MRPC | <a class="reference external" href="engine/examples/nlp/bert_base_mrpc/conf_fp32.yaml">conf_fp32.yaml</a> | <a class="reference external" href="engine/examples/nlp/bert_base_mrpc/conf_int8.yaml">conf_int8.yaml</a> | <a class="reference external" href="engine/examples/nlp/bert_base_mrpc/conf_int8_latency.yaml">conf_int8_latency.yaml</a> |
| Bert Large MLPerf | <a class="reference external" href="engine/examples/nlp/bert_large_mlperf/conf_fp32.yaml">conf_fp32.yaml</a> | <a class="reference external" href="engine/examples/nlp/bert_large_mlperf/conf_int8.yaml">conf_int8.yaml</a> | <a class="reference external" href="engine/examples/nlp/bert_large_mlperf/conf_int8_latency.yaml">conf_int8_latency.yaml</a> |
|
The *_latency.yaml is the model specially optimized for performance.</p>
<p>please remember to change the input data type, shape and range for your input in <code class="docutils literal notranslate"><span class="pre">inferencer.cpp</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="n">vector</span><span class="o">&lt;</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">input_dtype</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;int32&quot;</span><span class="p">);</span>
  <span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="nb">float</span><span class="o">&gt;&gt;</span> <span class="n">input_range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">vector</span><span class="o">&lt;</span><span class="nb">float</span><span class="o">&gt;</span><span class="p">({</span><span class="mi">1</span><span class="p">,</span> <span class="mi">300</span><span class="p">}));</span>
  <span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">int64_t</span><span class="o">&gt;&gt;</span> <span class="n">input_shape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">{</span><span class="n">FLAGS_batch_size</span><span class="p">,</span> <span class="n">FLAGS_seq_len</span><span class="p">});</span>
  <span class="n">executor</span><span class="p">::</span><span class="n">DataLoader</span><span class="o">*</span> <span class="n">dataloader</span><span class="p">;</span>
  <span class="o">//</span> <span class="n">dataloader</span> <span class="o">=</span> <span class="n">new</span> <span class="n">executor</span><span class="p">::</span><span class="n">ConstDataLoader</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">input_range</span><span class="p">);</span>
  <span class="n">dataloader</span> <span class="o">=</span> <span class="n">new</span> <span class="n">executor</span><span class="p">::</span><span class="n">DummyDataLoader</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">input_range</span><span class="p">);</span>
</pre></div>
</div>
<p>The dataloader generate data using prepare_batch() and make sure the generate data is the yaml need:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">:</span> 
  <span class="n">name</span><span class="p">:</span> <span class="n">bert_mlperf_int8</span>
  <span class="n">operator</span><span class="p">:</span>
    <span class="n">input_data</span><span class="p">:</span>
      <span class="nb">type</span><span class="p">:</span> <span class="n">Input</span>
      <span class="n">output</span><span class="p">:</span>
        <span class="c1"># -1 means it&#39;s dynamic shape</span>

        <span class="n">input_ids</span><span class="p">:</span>
          <span class="n">dtype</span><span class="p">:</span> <span class="n">int32</span>
          <span class="n">shape</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">segment_ids</span><span class="p">:</span> 
          <span class="n">dtype</span><span class="p">:</span> <span class="n">int32</span>
          <span class="n">shape</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">input_mask</span><span class="p">:</span>
          <span class="n">dtype</span><span class="p">:</span> <span class="n">int32</span>
          <span class="n">shape</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">softmax1_min</span><span class="p">:</span>
          <span class="n">dtype</span><span class="p">:</span> <span class="n">fp32</span>
          <span class="n">shape</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
          <span class="n">location</span><span class="p">:</span> <span class="p">[</span><span class="mi">430411380</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
</pre></div>
</div>
<p>All input tensors are in an operator typed Input. But slightly difference is some tensors have location while others not. A tensor with location means that is a frozen tensor or weight, it’s read from the bin file. A tensor without location means it’s activation, that should be input during model Forward. When you use C++ interface, initialize the tensor config and feed data/shape from dataloader:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="o">//</span> <span class="n">initialize</span> <span class="n">the</span> <span class="nb">input</span> <span class="n">tensor</span> <span class="n">config</span><span class="p">(</span><span class="n">which</span> <span class="n">correspond</span> <span class="n">to</span> <span class="n">the</span> <span class="n">yaml</span> <span class="n">tensor</span> <span class="n">without</span> <span class="n">location</span><span class="p">)</span>
  <span class="n">vector</span><span class="o">&lt;</span><span class="n">executor</span><span class="p">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">input_tensors</span><span class="p">;</span>
  <span class="n">auto</span> <span class="n">input_configs</span> <span class="o">=</span> <span class="n">bert_model</span><span class="o">.</span><span class="n">input_configs</span><span class="p">();</span>
  <span class="k">for</span> <span class="p">(</span><span class="nb">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">bert_model</span><span class="o">.</span><span class="n">num_inputs</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">input_tensors</span><span class="o">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">executor</span><span class="p">::</span><span class="n">Tensor</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">input_configs</span><span class="p">[</span><span class="n">i</span><span class="p">])));</span>
  <span class="o">//</span> <span class="n">feed</span> <span class="n">the</span> <span class="n">data</span> <span class="ow">and</span> <span class="n">shape</span>
  <span class="n">auto</span> <span class="n">raw_data</span> <span class="o">=</span> <span class="n">dataloader</span><span class="o">-&gt;</span><span class="n">prepare_batch</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
  <span class="k">for</span> <span class="p">(</span><span class="nb">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">input_tensors</span><span class="o">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">input_tensors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">raw_data</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="n">input_tensors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="p">}</span>
</pre></div>
</div>
<p>The output tensor is defined in an operator named Output, which only have inputs. Refer to <code class="docutils literal notranslate"><span class="pre">examples/execute_bert/conf_bert_mlperf_all_int8.yaml</span></code> and see:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="n">output_data</span><span class="p">:</span>
    <span class="nb">type</span><span class="p">:</span> <span class="n">Output</span>
    <span class="nb">input</span><span class="p">:</span>
      <span class="n">matmul_post_output_reshape</span><span class="p">:</span> <span class="p">{}</span>
</pre></div>
</div>
<p>You can add the tensor you want to the Output. Remember, all output tensors from operators should have operators take as input, that means output edges should have an end node. In this case, each operator’s output has one or several other operators take as input.</p>
<p>If you want to close log information of the <code class="docutils literal notranslate"><span class="pre">inferencer</span></code>, use the command <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">GLOG_minloglevel=2</span></code> before executing the <code class="docutils literal notranslate"><span class="pre">inferencer</span></code>.  <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">GLOG_minloglevel=1</span></code> will open the log information again. This command can also be used in python engine model.</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Intel® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>